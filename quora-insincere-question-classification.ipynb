{"cells":[{"metadata":{"_uuid":"40d303d1d140eb269fee05f9ddb7e29d41e00d73"},"cell_type":"markdown","source":"## Setting Up"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport re\n\n# Use tqdm to show progress of an pandas function we use\ntqdm.pandas()\n\nfrom gensim.models import KeyedVectors as kv\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nembedding_path_dict= {'googlenews':{\n                            'path':'../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin',\n                            'format':'word2vec',\n                            'binary': True\n                      },\n                      'glove':{\n                            'path':'../input/embeddings/glove.840B.300d/glove.840B.300d.txt',\n                            'format': 'glove',\n                            'binary': ''\n                      },\n                      'glove_word2vec':{\n                            'path':'../input/glove.840B.300d.txt.word2vec',\n                            'format': 'word2vec',\n                            'binary': False\n                      },\n                      'wiki':{\n                            'path': '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec',\n                            'format': 'word2vec',\n                            'binary': False\n                      },\n                      'paragram':{\n                            'path': '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt',\n                            'format': '',\n                            'binary': False\n                      }\n                    }\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de1615767f5c4de575679313d2b7983f032c345a"},"cell_type":"markdown","source":"## Get Training and Test Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/train.csv\")\ntest= pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e31b085ffe907fdf8d072f319baf43b8baf272ca"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ee00b7e5db1b212400206b5d73a91930e0bfa84"},"cell_type":"code","source":"train = train.loc[train.question_text.str.len()>100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70a090cedfeb97764bfc14dfbf63c03ab40f8a81"},"cell_type":"code","source":"len(train.loc[train['target']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8bcad6f51d6ef07a8c66b43b30222e8a4d5943b"},"cell_type":"code","source":"num_pos= len(train.loc[train['target']==1])\nprint(num_pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3962dd3f63820949c27ceb5580df6aa5059f1fa9","scrolled":true},"cell_type":"code","source":"len(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99c49a6d2a9c1ae777127255e7c16f1fed63c897"},"cell_type":"markdown","source":"Training set positive and negative examples are very unbalanced"},{"metadata":{"trusted":true,"_uuid":"046b2358caf89b0753cea59f05ad673a9d72192a"},"cell_type":"code","source":"balanced_train= train.loc[train['target']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e73deac6a08847708eeae71580517f18235a3885"},"cell_type":"code","source":"balanced_train = balanced_train.append(train.loc[train['target']==0].sample(n=num_pos), ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41d2c9f8b940b926f52f5b84982907cc6824c12c"},"cell_type":"code","source":"len(balanced_train.loc[balanced_train['target']==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5484f6742393d36c553eca081800ccaadb00d7d"},"cell_type":"code","source":"del train\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"782b237b993dc687620e200526fac4328b200e23"},"cell_type":"markdown","source":"## Choose Word Embeddings"},{"metadata":{"_uuid":"c22655d390c33bdb1923493609549920a383e649"},"cell_type":"markdown","source":"### Functions: Embedding-Related Functions"},{"metadata":{"trusted":true,"_uuid":"b526d8f9de87a2195727d43df0658d540ba416fb","_kg_hide-input":false},"cell_type":"code","source":"# Get word embeddings\ndef get_embeddings(embedding_path_dict, emb_name):\n    \"\"\"\n    :params embedding_path_dict: a dictionary containing the path, binary flag, and format of the desired embedding,\n            emb_name: the name of the embedding to retrieve\n    :return embedding index: a dictionary containing the embeddings\"\"\"\n    \n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    \n    embeddings_index = {}\n    if (emb_name == 'googlenews'):\n        emb_path = embedding_path_dict[emb_name]['path']\n        bin_flag = embedding_path_dict[emb_name]['binary']\n        embeddings_index = kv.load_word2vec_format(emb_path, binary=bin_flag).vectors\n    elif (emb_name in ['glove', 'wiki']):\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path_dict[emb_name]['path']) if len(o)>100)    \n    elif (emb_name == 'paragram'):\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path_dict[emb_name]['path'], encoding=\"utf8\", errors='ignore'))\n    return embeddings_index\n\n#Convert GLoVe format into word2vec format\ndef glove_to_word2vec(embedding_path_dict, emb_name='glove', output_emb='glove_word2vec'):\n    \"\"\"\n    Convert the GLOVE embedding format to a word2vec format\n    :params embedding_path_dict: a dictionary containing the path, binary flag, and format of the desired embedding,\n            glove_path: the name of the GLOVE embedding\n            output_file_path: the name of the converted embedding in embedding_path_dict. \n    :return output from the glove2word2vec script\n    \"\"\"\n    glove_input_file = embedding_path_dict[emb_name]['path']\n    word2vec_output_file = embedding_path_dict[output_emb]['path']                \n    return glove2word2vec(glove_input_file, word2vec_output_file)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc88daa8454e41983bfa890253a7c4ce60e4dc9c"},"cell_type":"code","source":"# Get stats of a given embeddings index\ndef get_emb_stats(embeddings_index):\n\n    # Put all embeddings in a numpy matrix\n    all_embs= np.stack(embeddings_index.values())\n\n    # Get embedding stats\n    emb_mean = all_embs.mean()\n    emb_std = all_embs.std()\n    \n    num_embs = all_embs.shape[0]\n    \n    emb_size = all_embs.shape[1]\n    \n    return emb_mean,emb_std, num_embs, emb_size ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"098bd88715aacc048326d6ed766eb816556d5634"},"cell_type":"markdown","source":"### Functions: Tokenize Training Sentences"},{"metadata":{"trusted":true,"_uuid":"c279ced2e2ad7e249cb0bc6218fbe6bdb6c101f0"},"cell_type":"code","source":"contr_dict={\"I\\'m\": \"I am\",\n            \"won\\'t\": \"will not\",\n            \"\\'s\" : \"\", \n            \"\\'ll\":\"will\",\n            \"\\'ve\":\"have\",\n            \"n\\'t\":\"not\",\n            \"\\'re\": \"are\",\n            \"\\'d\": \"would\",\n            \"y'all\": \"all of you\",\n            \"Quoran\": \"Quora contributor\",\n            \"quoran\": \"quora contributor\"\n            }\n\n# Converts sentences into lists of tokens\n# We use this function to allow more control over what constitutes a word\n# It also allows us to explore ways to cover more the pre-defined word embeddings.\n\ndef tokenize(sentences, restrict_to_len=-1):\n    \"\"\"\n    :params sentence_list: list of strings\n    :returns tok_sentences: list of list of tokens\n    \"\"\"\n    \n    if restrict_to_len>0:\n        tok_sentences = [re.findall(r\"[\\w]+[']*[\\w]+|[\\w]+|[.,!?;]\", x ) \\\n                         for x in sentences if len(x)>restrict_to_len]\n    else:\n        tok_sentences = [re.findall(r\"[\\w]+[']*[\\w]+|[\\w]+|[.,!?;]\", x ) \\\n                         for x in sentences] \n    return tok_sentences\n\n#Build the vocabulary given a list of sentence words\ndef get_vocab(sentences, verbose= True):\n    \"\"\"\n    :param sentences: a list of list of words\n    :return: a dictionary of words and their frequency \n    \"\"\"\n    vocab={}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] +=1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef repl(m):\n    return '#' * len(m.group())\n\n#Convert numerals to a # sign\ndef convert_num_to_pound(sentences):\n    return sentences.progress_apply(lambda x: re.sub(\"[1-9][\\d]+\", repl, x)).values\n\ndef replace_contractions(sentences, contr_dict=contr_dict):\n    res_sentences=[]\n    for sent in sentences:\n        for contr in contr_dict:\n            sent = sent.replace(contr, \" \"+contr_dict[contr])\n        res_sentences.append(sent)\n    return res_sentences\n\ndef convert_height(sentences):\n    res_sentences = []\n    for sent in sentences:\n        res_sent = re.sub( \"(\\d+)\\'(\\d+)\", \"\\1 foot \\2\", sent)\n        res_sentences.append(res_sent)\n    return res_sentences\n\ndef convert_to_lower(sentences):\n    res_sentences = []\n    for sent in sentences:\n        lower_sent = sent.lower()\n        res_sentences.append(lower_sent)\n    return res_sentences\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1b9aeb8c417055d5c7aa97cd2dcdf0b33681f31"},"cell_type":"markdown","source":"### Functions: Train for an embedding"},{"metadata":{"trusted":true,"_uuid":"f6dc8ace1fab6525cfe9ba93adc59c8fd37305ff"},"cell_type":"code","source":"def get_emb_matrix(sentences, embeddings_index, emb_mean, emb_std,\\\n                   emb_size, max_num_tokens = 300000 ):\n    \n    # max_num_tokens id Vocabulary size limit\n    vocab = get_vocab(sentences)\n\n    # maximum vocabulary size\n    num_words = min(max_num_tokens, len(vocab.keys()))\n\n    # words not in pre-trained embedding are given freature values \n    # drawn from a normal distribution with emb_mean and emb_std\n    # This initialization is less random than initializing with 0.\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, emb_size))\n\n    # Get embeddings of training vocabulary\n    for i, word in enumerate(vocab.keys()):\n        if i >= max_num_tokens: continue\n        if (word in embeddings_index):\n            embedding_vector = embeddings_index[word]\n            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    # we don't use vocab later\n    del vocab\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82916c0c5ab36b250b25cd6c6ac0b9fc449f8d13"},"cell_type":"code","source":"def choose_emb_and_train(embedding_name,train, maxlen=100): #train should be balanced_train\n    \n    # get the embeddings\n    embeddings_index= get_embeddings(embedding_path_dict, embedding_name)\n\n    # Get embedding stats\n    emb_mean,emb_std, num_embs, emb_size = get_emb_stats(embeddings_index)\n    print(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n          %(emb_mean,emb_std, num_embs, emb_size))\n    \n    # Tokenize training set\n    if (embedding_name == 'googlenews'):\n        # Google replaces digits in numbers > 9 with # signs\n        question_text = convert_num_to_pound(train[\"question_text\"])\n    else:\n        question_text = train[\"question_text\"]\n\n    # start by replacing heights such as 5'4 to a longer format (5 foot 4)\n    sentences = convert_height(question_text)\n\n    # No need to convert capitals to lower case for GloVe as it has both in its embeddings\n    if (embedding_name == 'paragram'):\n        # convert capitals to lowercase\n        sentences = convert_to_lower(sentences)\n\n    # replace contractions\n    sentences = replace_contractions(sentences)\n\n    # Get a list of token for each question text\n    # restrict_to_len is approximately the mean sentence length+ 0.5std\n    sentences = tokenize(sentences, restrict_to_len=maxlen)\n    \n    # get embeddings matrix\n    embedding_matrix = get_emb_matrix(sentences, embeddings_index, emb_mean, emb_std,\\\n                   emb_size, max_num_tokens = 300000 )\n    \n    return embedding_matrix,emb_mean,emb_std, num_embs, emb_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b329e1e003a5890268ce3cea99b5f4d95e95d94"},"cell_type":"code","source":"import statistics as st\n\n# Get the mean, median, and maximum question length, as well as the standard deviation\ndef get_set_stats(given_sent_set):\n    question_len=tqdm([len(x) for x in given_sent_set])\n    maxlen= max(question_len)\n    minlen= min(question_len)\n    mean_len = st.mean(question_len)\n    std_len = st.stdev(question_len)\n    median = st.median(question_len)\n\n    return maxlen, minlen, mean_len, std_len, median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33556f924d8026da348bf9cc9253a89dde2a5dc2"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\ndef preprocess_input_sets(input_set, keras_tokenizer,\\\n                          num_words=0, maxlen=300, \\\n                          test=False, lower=False,\\\n                          conv_height=False, contractions=False):\n    # Get train and val text\n    X = input_set[\"question_text\"].fillna(\"_na_\").values\n    \n    if conv_height:\n        # start by replacing heights such as 5'4 to a longer format (5 foot 4)\n        X = convert_height(X)\n\n    if lower:\n        #convert capitals to lower case\n        X = convert_to_lower(X)\n\n    if contractions:\n        # replace contractions\n        X = replace_contractions(X)\n\n    Y=None\n    if not(test):\n        # Get target classes\n        Y = input_set[\"target\"].fillna(\"_na_\").values\n\n    if (keras_tokenizer is None):\n        if (num_words > 0):\n            keras_tokenizer = Tokenizer(num_words= num_words)\n            keras_tokenizer.fit_on_texts(list(X))\n        else:\n            print(\"Num words is required to create Keras Tokenizer object\")\n            return None, None, None\n\n    try:\n        X = keras_tokenizer.texts_to_sequences(X)\n    except NameError:\n        print(\"Tokenizer object not defined!\")\n        return None, None\n\n    ## Pad the sentences \n    X = pad_sequences(X, maxlen=maxlen)\n    \n    return X, Y, keras_tokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ca3af06ba387c11e12f5582a651419fb25f2b14"},"cell_type":"markdown","source":"## Model-related functions"},{"metadata":{"trusted":true,"_uuid":"61b39035feb3a05c774ec01917d13c65f381129d"},"cell_type":"code","source":"def build_model (embedding_matrix, emb_size=300, max_len=100, voc_size=50000):\n    input = Input(shape=(max_len,))\n    embed = Embedding(voc_size, emb_size, weights=[embedding_matrix])(input)\n    \n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(embed)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(16, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    \n    y = Bidirectional(CuDNNGRU(64, return_sequences=True))(embed)\n    y = Bidirectional(CuDNNLSTM(64, return_sequences=True))(y)\n    y = GlobalMaxPool1D()(y)\n    y = Dense(16, activation=\"relu\")(y)\n    y = Dropout(0.1)(y)\n    \n    z= Concatenate()([x,y])\n    \n    output = Dense(1, activation=\"sigmoid\")(z)\n    \n    model = Model (inputs=input, outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f73413888e943887ebd065002e42640278e305ac"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\ndef train_network(model_name, model, train_X, train_Y,\\\n                  val_X, val_Y, \\\n                  batch_size = 1500, epochs = 10,\\\n                  monitor='val_loss', mode='min'):\n    \n    early_stopping = EarlyStopping(patience=3, verbose=1, monitor=monitor, mode=mode)\n    model_checkpoint = ModelCheckpoint(model_name, save_best_only=True, verbose=1, \\\n                                       monitor=monitor, mode=mode)\n    reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n    hist = model.fit(train_X, train_Y, batch_size=batch_size, epochs=epochs,\\\n                     validation_data=(val_X, val_Y), verbose=True, \\\n                     callbacks=[early_stopping,model_checkpoint, reduce_lr])\n\n    return hist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0daa6c2016297a11d6338f65fd27982c823cd3c"},"cell_type":"markdown","source":"### GloVe\n\n#### Get embeddings and  Create Training Embedding Matrices"},{"metadata":{"trusted":true,"_uuid":"8ba04dbc07c3bb70952b96a553a6d10c2a0ec4a1"},"cell_type":"code","source":"maxlen = 100\nnum_words=50000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b391d6f4c4a59b43da41512eb300b1d6a586e376"},"cell_type":"code","source":"embedding_matrix,emb_mean,emb_std, num_embs, emb_size = \\\nchoose_emb_and_train('glove',balanced_train, maxlen=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8d3f9d989ce99521d66aa38eb8657b39555cb1","scrolled":true},"cell_type":"code","source":"# print embedding stats\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337edb05545f04aad8751790dbef9adcd82f917d"},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413a76cfb49cb97b6d04721e562652104a1f6bd7"},"cell_type":"markdown","source":"#### Create Training and  Validation sets"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"01d39edcd32b188099495a98a1d43c79c7f5e936"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntraining_set, val_set = train_test_split(balanced_train, test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdea6421b6b1f2d17fdd04c84f90e2cb6331ade"},"cell_type":"markdown","source":"### Get training, test, and val set stats"},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_train, minlen_train, mean_len_train, std_len_train, median_train = get_set_stats(training_set[\"question_text\"])\nprint(\"Question Length Stats in Training set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_train, minlen_train, mean_len_train, median_train, std_len_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_val, minlen_val, mean_len_val, std_len_val, median_val = get_set_stats(val_set[\"question_text\"])\nprint(\"Question Length Stats in Validation set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_val, minlen_val, mean_len_val, median_val, std_len_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_test, minlen_test, mean_len_test, std_len_test, median_test = get_set_stats(test[\"question_text\"])\nprint(\"Question Length Stats in Test set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_test, minlen_test, mean_len_test, median_test, std_len_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a95b52e96358b6e70e3f2cc881eecfd837ae3825"},"cell_type":"markdown","source":"Hmmm... Same mean, median, and std but different size ranges! Needs more investigation (To Do)"},{"metadata":{"_uuid":"57202d0ac140ecdd79d764adb469f84de56fba4a"},"cell_type":"markdown","source":"### Tokenize, preprocess, and pad training, val, and test sets"},{"metadata":{"trusted":true,"_uuid":"801b1557736bb171a7b3e1faaffefac303ce3df2"},"cell_type":"code","source":"# Convert token lists into sequences\ntrain_X, train_Y, keras_tokenizer = preprocess_input_sets(training_set, None,\\\n                                                          num_words=50000,\\\n                                                          maxlen=maxlen, \\\n                                                          lower=False,\\\n                                                          conv_height=True, \\\n                                                          contractions=True)\nval_X, val_Y, _ = preprocess_input_sets(val_set, keras_tokenizer,lower=False,\\\n                         conv_height=True, contractions=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a6b1771f77e2492dba965b78feff17069a0ff94"},"cell_type":"markdown","source":"## Model"},{"metadata":{"_uuid":"f664272c0ee6cdaf5c84d01ba6626c48b6763aae"},"cell_type":"markdown","source":"### Build the network"},{"metadata":{"trusted":true,"_uuid":"ab44f766aa90ae99cc3aebbf264035c30ab80776"},"cell_type":"code","source":"# Free up memory\n#del embeddings_index\n#import time, gc; gc.collect()\n#time.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78ede2c79292e6a0319871cd8db7b50ddaa6ef6f"},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, LSTM, Dense, Input\nfrom keras.layers import Flatten\nfrom keras.layers import CuDNNLSTM, CuDNNGRU, Concatenate, Dense,  Dropout\nfrom keras.models import Model\n\n\ndata_dim = 16\ntimesteps = 8\nnum_classes = 1 #10\nbatch_size = 32\nnum_mem_units = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78ede2c79292e6a0319871cd8db7b50ddaa6ef6f"},"cell_type":"code","source":"import os\n\nmodel_name='./glove_model.model'\nif (os.path.isfile(model_name)):\n    model = load(model_name)\nelse:\n    model = build_model(embedding_matrix,  emb_size = embedding_matrix.shape[1],\\\n                        max_len=maxlen, voc_size=embedding_matrix.shape[0])\n    model.compile(loss='binary_crossentropy',   \n                  optimizer='adam', #rmsprop',            \n                  metrics=['accuracy'])\n\n# summarize the model\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38778371bf1ede2aec4ef6b62dd18d7ee7081cf"},"cell_type":"markdown","source":"### Train the network"},{"metadata":{"trusted":true,"_uuid":"d3a78d062b7bbbb9e0a2ee61c3a0e2e3bef80c04"},"cell_type":"code","source":"hist = train_network(model_name, model, train_X, train_Y,\\\n                      val_X, val_Y, \\\n                      batch_size = 1500, epochs = 10,\\\n                      monitor='val_loss', mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537f9cb14da475c1d15cb723b67af96dfac08ab3"},"cell_type":"code","source":"#model = load_model('./glove_model.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd69d2cd4dcee520eda72463bc1d085c091acd9e"},"cell_type":"code","source":"print(\"total preds:%d negative:%d pos:%d\\n\"% (len(val_Y), np.count_nonzero(val_Y), len(val_Y)-np.count_nonzero(val_Y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c43bee8264a18c664a986ccea4830b44b054ed91"},"cell_type":"markdown","source":"### Find best threshold cutoff"},{"metadata":{"trusted":true,"_uuid":"1944b0587af766539d6cdbff5f7512f18c8dcb66"},"cell_type":"code","source":"from sklearn import metrics\n\npred_val_Y= model.predict([val_X], batch_size=batch_size, verbose=1)\n\nmax_f1 = 0.0\nmax_thresh = 0.0\nfor thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        curr_f1 = metrics.f1_score(val_Y, (pred_val_Y>thresh).astype(int))\n        if max_f1>curr_f1:\n            max_f1 = curr_f1\n            max_thresh = thresh\n        print(\"Threshold:%1.2f F1 Score:%5.5f\"%(thresh, curr_f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"add365f1838882dbe5fe3d0ce450fb30085945d3"},"cell_type":"markdown","source":"## Run on Test"},{"metadata":{"trusted":true,"_uuid":"1679dfff23553de22ffa3860fd4481d6815b4682"},"cell_type":"code","source":"test= pd.read_csv(\"../input/test.csv\")\nprint(\"Test shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01e4b8f5398cd5f2bb70ead8620959258ac09fc"},"cell_type":"code","source":"pred_X, _, _ = preprocess_input_sets(test, keras_tokenizer, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfe47be7ca2258d6ab72ef14addb0582f514406b"},"cell_type":"code","source":"pred_Y= model.predict([pred_X], batch_size=1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"978f599ad4969c22ad539d80d28c75345dc550f0"},"cell_type":"code","source":"len(pred_Y[pred_Y<0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"9c9d18563600e331ae3793e0c61825f009bbcc24"},"cell_type":"code","source":"len(pred_Y[pred_Y>=0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c225e217e09a6d1f77d16c3fcba357cb87348bcc"},"cell_type":"code","source":"tmp_pred = (pred_Y>(0.5)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b84c2149bdca7774120b7d038c25e256f3f496"},"cell_type":"code","source":"len(tmp_pred[tmp_pred==0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31408588b3a4eb12cbe72a5a0835a80624a2f63c"},"cell_type":"markdown","source":"### Save Test"},{"metadata":{"trusted":true,"_uuid":"a9b540879105dc2dfc6aafbcd09b141154c810f0"},"cell_type":"code","source":"len(pred_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3f651ecc467697cc927b6d0cbd5becae98948be1"},"cell_type":"code","source":"len(pred_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e3d7e4a3cca736d84d4178a5c047a4127d5839"},"cell_type":"code","source":"len(test[\"qid\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa8e2026b54d46c7f4b6a0806458c085845eda6"},"cell_type":"code","source":"test_res= pd.DataFrame({\"qid\":test[\"qid\"].values})\ntest_pred = (pred_Y>0.5).astype(int)\ntest_res['prediction'] = test_pred\ntest_res.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5c4133eaa63719b9cc708125e83e2b58d549143","scrolled":true},"cell_type":"code","source":"#test_res.to_csv(\"glove_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04a933a323aa533f93f012d0234957b9dcdb0e9"},"cell_type":"markdown","source":"## LSTM with Paragrams Embeddings"},{"metadata":{"_uuid":"883b4bdc1907ef695b7c9c83cd5c63812d8b365f"},"cell_type":"markdown","source":"### Get Embeddings Index and  Tokenize Training Sentences"},{"metadata":{"trusted":true,"_uuid":"3e1430e174dbd7dcd8f3d05b13c7643cf8ab8d19"},"cell_type":"code","source":"embedding_name = 'paragram'\nembedding_matrix,emb_mean,emb_std, num_embs, emb_size = \\\nchoose_emb_and_train(embedding_name,balanced_train, maxlen=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8d3f9d989ce99521d66aa38eb8657b39555cb1","scrolled":true},"cell_type":"code","source":"# print embedding stats\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337edb05545f04aad8751790dbef9adcd82f917d"},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413a76cfb49cb97b6d04721e562652104a1f6bd7"},"cell_type":"markdown","source":"#### Create Training and  Validation sets"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"01d39edcd32b188099495a98a1d43c79c7f5e936"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntraining_set, val_set = train_test_split(balanced_train, test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdea6421b6b1f2d17fdd04c84f90e2cb6331ade"},"cell_type":"markdown","source":"### Get training, test, and val set stats"},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_train, minlen_train, mean_len_train, std_len_train, median_train = get_set_stats(training_set[\"question_text\"])\nprint(\"Question Length Stats in Training set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_train, minlen_train, mean_len_train, median_train, std_len_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_val, minlen_val, mean_len_val, std_len_val, median_val = get_set_stats(val_set[\"question_text\"])\nprint(\"Question Length Stats in Validation set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_val, minlen_val, mean_len_val, median_val, std_len_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_test, minlen_test, mean_len_test, std_len_test, median_test = get_set_stats(test[\"question_text\"])\nprint(\"Question Length Stats in Test set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_test, minlen_test, mean_len_test, median_test, std_len_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0154790f62a2207a1dcb90cdacb5e92fd1b98010"},"cell_type":"markdown","source":"### Tokenize, preprocess, and pad training, val, and test sets"},{"metadata":{"trusted":true,"_uuid":"d76d31a591c9950c7fb4f25c87c8cdc2f52eba6c"},"cell_type":"code","source":"# Convert token lists into sequences\ntrain_X, train_Y, keras_tokenizer = preprocess_input_sets(training_set, None, num_words, \\\n                                                         lower=True,\\\n                                                         conv_height=True, \\\n                                                         contractions=True)\nval_X, val_Y, _ = preprocess_input_sets(val_set, keras_tokenizer,lower=True,\\\n                         conv_height=True, contractions=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9e3eb91c15ddbd0af7d1c7c3a68f1c54907099"},"cell_type":"markdown","source":"### Build the Network"},{"metadata":{"trusted":true,"_uuid":"200451cd96287e982049973af69e1ffc014108d0"},"cell_type":"code","source":"model_name='./para_model.model'\nif (os.path.isfile(model_name)):\n    model = load(model_name)\nelse:\n    model_p = build_model(embedding_matrix, emb_size = embedding_matrix.shape[1],\\\n                    max_len=maxlen, voc_size=embedding_matrix.shape[0])\n    model.compile(loss='binary_crossentropy',   \n                  optimizer='adam', #rmsprop',            \n                  metrics=['accuracy'])\n# summarize the model\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"830e7394bf85efeab3b70884239d81b9cfcdaa75"},"cell_type":"markdown","source":"### Train the Network"},{"metadata":{"trusted":true,"_uuid":"6a6c94dc4cb4f10faa0821ba37251e2946de680a"},"cell_type":"code","source":"hist = train_network(model_name, model, train_X, train_Y,\\\n                  val_X, val_Y, \\\n                  batch_size = 1500, epochs = 10,\\\n                  monitor='val_loss', mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd69d2cd4dcee520eda72463bc1d085c091acd9e"},"cell_type":"code","source":"print(\"total preds:%d negative:%d pos:%d\\n\"% (len(val_Y), np.count_nonzero(val_Y), len(val_Y)-np.count_nonzero(val_Y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c43bee8264a18c664a986ccea4830b44b054ed91"},"cell_type":"markdown","source":"### Find best threshold cutoff"},{"metadata":{"trusted":true,"_uuid":"1944b0587af766539d6cdbff5f7512f18c8dcb66"},"cell_type":"code","source":"from sklearn import metrics\n\npara_pred_val_Y= model.predict([val_X], batch_size=batch_size, verbose=1)\n\nmax_f1 = 0.0\nmax_thresh = 0.0\nfor thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        curr_f1 = metrics.f1_score(val_Y, (para_pred_val_Y>thresh).astype(int))\n        if max_f1>curr_f1:\n            max_f1 = curr_f1\n            max_thresh = thresh\n        print(\"Threshold:%1.2f F1 Score:%5.5f\"%(thresh, curr_f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"add365f1838882dbe5fe3d0ce450fb30085945d3"},"cell_type":"markdown","source":"## Run on Test"},{"metadata":{"trusted":true,"_uuid":"1679dfff23553de22ffa3860fd4481d6815b4682"},"cell_type":"code","source":"test= pd.read_csv(\"../input/test.csv\")\nprint(\"Test shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01e4b8f5398cd5f2bb70ead8620959258ac09fc"},"cell_type":"code","source":"para_pred_X, _, _ = preprocess_input_sets(test, keras_tokenizer, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfe47be7ca2258d6ab72ef14addb0582f514406b"},"cell_type":"code","source":"para_pred_Y= model.predict([para_pred_X], batch_size=1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5f0b1827ce730e783ec524cccf5f6faaa7912c0"},"cell_type":"code","source":"len(para_pred_Y[para_pred_Y<0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e32a1a7bdd57078e99c53324bcbd758634c0d9d8"},"cell_type":"code","source":"len(para_pred_Y[para_pred_Y>=0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"212ce66de921227dc8ba3d0b480cccf8d43d7fe0"},"cell_type":"code","source":"tmp_pred = (para_pred_Y>(0.5)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c50e7483533f90dd7980d5880150d67061735c"},"cell_type":"code","source":"len(tmp_pred[tmp_pred==0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31408588b3a4eb12cbe72a5a0835a80624a2f63c"},"cell_type":"markdown","source":"### Save Test"},{"metadata":{"trusted":true,"_uuid":"a9b540879105dc2dfc6aafbcd09b141154c810f0"},"cell_type":"code","source":"len(para_pred_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3f651ecc467697cc927b6d0cbd5becae98948be1"},"cell_type":"code","source":"len(para_pred_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e3d7e4a3cca736d84d4178a5c047a4127d5839"},"cell_type":"code","source":"len(test[\"qid\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa8e2026b54d46c7f4b6a0806458c085845eda6"},"cell_type":"code","source":"para_test_res= pd.DataFrame({\"qid\":test[\"qid\"].values})\npara_test_pred = (para_pred_Y>0.5).astype(int)\npara_test_res['prediction'] = para_test_pred\npara_test_res.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f26e169929fb43411a55013037b8ab4d8a58316"},"cell_type":"code","source":"#test_res.to_csv(\"para_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04a933a323aa533f93f012d0234957b9dcdb0e9"},"cell_type":"markdown","source":"## LSTM with Wiki Embeddings"},{"metadata":{"_uuid":"883b4bdc1907ef695b7c9c83cd5c63812d8b365f"},"cell_type":"markdown","source":"### Get Wiki Embeddings Index"},{"metadata":{"trusted":true,"_uuid":"e136ce606f51aa2a6b044bde58295445660c870a"},"cell_type":"code","source":"embedding_name = 'wiki'\nembedding_matrix,emb_mean,emb_std, num_embs, emb_size = \\\nchoose_emb_and_train(embedding_name,balanced_train, maxlen=100)\nimport gc; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8d3f9d989ce99521d66aa38eb8657b39555cb1","scrolled":false},"cell_type":"code","source":"# Get embedding stats\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337edb05545f04aad8751790dbef9adcd82f917d"},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413a76cfb49cb97b6d04721e562652104a1f6bd7"},"cell_type":"markdown","source":"#### Create Training and  Validation sets"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"01d39edcd32b188099495a98a1d43c79c7f5e936"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntraining_set, val_set = train_test_split(balanced_train, test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdea6421b6b1f2d17fdd04c84f90e2cb6331ade"},"cell_type":"markdown","source":"### Get training, test, and val set stats"},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_train, minlen_train, mean_len_train, std_len_train, median_train = get_set_stats(training_set[\"question_text\"])\nprint(\"Question Length Stats in Training set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_train, minlen_train, mean_len_train, median_train, std_len_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_val, minlen_val, mean_len_val, std_len_val, median_val = get_set_stats(val_set[\"question_text\"])\nprint(\"Question Length Stats in Validation set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_val, minlen_val, mean_len_val, median_val, std_len_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f6e668368bf352a75c8e74d22fddda729f435c"},"cell_type":"code","source":"maxlen_test, minlen_test, mean_len_test, std_len_test, median_test = get_set_stats(test[\"question_text\"])\nprint(\"Question Length Stats in Test set:\\n\")\nprint(\"\\tmaximum:%d\\n\\tminimum:%d\\n\\tmean:%d\\n\\tmedian:%d\\n\\tstd:%d\"% \\\n      (maxlen_test, minlen_test, mean_len_test, median_test, std_len_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0154790f62a2207a1dcb90cdacb5e92fd1b98010"},"cell_type":"markdown","source":"### Tokenize, preprocess, and pad training, val, and test sets"},{"metadata":{"trusted":true,"_uuid":"d76d31a591c9950c7fb4f25c87c8cdc2f52eba6c"},"cell_type":"code","source":"# Convert token lists into sequences\ntrain_X, train_Y, keras_tokenizer = preprocess_input_sets(training_set, None, num_words, \\\n                                                         lower=True,\\\n                                                         conv_height=True, \\\n                                                         contractions=True)\nval_X, val_Y, _ = preprocess_input_sets(val_set, keras_tokenizer,lower=True,\\\n                         conv_height=True, contractions=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9e3eb91c15ddbd0af7d1c7c3a68f1c54907099"},"cell_type":"markdown","source":"### Build the Network"},{"metadata":{"trusted":true,"_uuid":"200451cd96287e982049973af69e1ffc014108d0"},"cell_type":"code","source":"model_name='./wiki_model.model'\n\nif (os.path.isfile(model_name)):\n    model = load(model_name)\nelse:\n    model_p = build_model(embedding_matrix, emb_size = embedding_matrix.shape[1],\\\n                    max_len=maxlen, voc_size=embedding_matrix.shape[0])\n    model.compile(loss='binary_crossentropy',   \n                  optimizer='adam', #rmsprop',            \n                  metrics=['accuracy'])\n# summarize the model\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"830e7394bf85efeab3b70884239d81b9cfcdaa75"},"cell_type":"markdown","source":"### Train the Network"},{"metadata":{"trusted":true,"_uuid":"d3a78d062b7bbbb9e0a2ee61c3a0e2e3bef80c04"},"cell_type":"code","source":"hist = train_network(model_name, model, train_X, train_Y,\\\n                  val_X, val_Y, \\\n                  batch_size = 1500, epochs = 10,\\\n                  monitor='val_loss', mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd69d2cd4dcee520eda72463bc1d085c091acd9e"},"cell_type":"code","source":"print(\"total preds:%d negative:%d pos:%d\\n\"% (len(val_Y), np.count_nonzero(val_Y), len(val_Y)-np.count_nonzero(val_Y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c43bee8264a18c664a986ccea4830b44b054ed91"},"cell_type":"markdown","source":"### Find best threshold cutoff"},{"metadata":{"trusted":true,"_uuid":"1944b0587af766539d6cdbff5f7512f18c8dcb66"},"cell_type":"code","source":"from sklearn import metrics\n\nwiki_pred_val_Y= model.predict([val_X], batch_size=batch_size, verbose=1)\n\nmax_f1 = 0.0\nmax_thresh = 0.0\nfor thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        curr_f1 = metrics.f1_score(val_Y, (wiki_pred_val_Y>thresh).astype(int))\n        if max_f1>curr_f1:\n            max_f1 = curr_f1\n            max_thresh = thresh\n        print(\"Threshold:%1.2f F1 Score:%5.5f\"%(thresh, curr_f1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"add365f1838882dbe5fe3d0ce450fb30085945d3"},"cell_type":"markdown","source":"## Run on Test"},{"metadata":{"trusted":true,"_uuid":"1679dfff23553de22ffa3860fd4481d6815b4682"},"cell_type":"code","source":"test= pd.read_csv(\"../input/test.csv\")\nprint(\"Test shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01e4b8f5398cd5f2bb70ead8620959258ac09fc"},"cell_type":"code","source":"wiki_pred_X, _, _ = preprocess_input_sets(test, keras_tokenizer, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfe47be7ca2258d6ab72ef14addb0582f514406b"},"cell_type":"code","source":"wiki_pred_Y= model.predict([wiki_pred_X], batch_size=1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"978f599ad4969c22ad539d80d28c75345dc550f0"},"cell_type":"code","source":"len(wiki_pred_Y[wiki_pred_Y<0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"9c9d18563600e331ae3793e0c61825f009bbcc24"},"cell_type":"code","source":"len(wiki_pred_Y[wiki_pred_Y>=0.5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c225e217e09a6d1f77d16c3fcba357cb87348bcc"},"cell_type":"code","source":"tmp_pred = (wiki_pred_Y>(0.5)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b84c2149bdca7774120b7d038c25e256f3f496"},"cell_type":"code","source":"len(tmp_pred[tmp_pred==0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31408588b3a4eb12cbe72a5a0835a80624a2f63c"},"cell_type":"markdown","source":"### Save Test"},{"metadata":{"trusted":true,"_uuid":"a9b540879105dc2dfc6aafbcd09b141154c810f0"},"cell_type":"code","source":"len(pred_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3f651ecc467697cc927b6d0cbd5becae98948be1"},"cell_type":"code","source":"len(wiki_pred_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4e3d7e4a3cca736d84d4178a5c047a4127d5839"},"cell_type":"code","source":"len(test[\"qid\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa8e2026b54d46c7f4b6a0806458c085845eda6"},"cell_type":"code","source":"test_res= pd.DataFrame({\"qid\":test[\"qid\"].values})\ntest_pred = (wiki_pred_Y>0.5).astype(int)\ntest_res['prediction'] = test_pred\ntest_res.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f26e169929fb43411a55013037b8ab4d8a58316"},"cell_type":"code","source":"#test_res.to_csv(\"wiki_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4435d50b22e447d9b6b515f66e22d8941b2f710e"},"cell_type":"markdown","source":"## Combine Results"},{"metadata":{"trusted":true,"_uuid":"16af1223f9a3f955d07f95ed38106e7089bc346e"},"cell_type":"code","source":"ens_val_pred = (0.33 * wiki_pred_val_Y) + (0.33 * para_pred_val_Y) + (0.33 * pred_val_Y)\n\nmax_f1 = 0.0\nmax_thresh = 0.0\nfor thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        curr_f1 = metrics.f1_score(val_Y, (ens_val_pred>thresh).astype(int))\n        if max_f1>curr_f1:\n            max_f1 = curr_f1\n            max_thresh = thresh\n        print(\"Threshold:%1.2f F1 Score:%5.5f\"%(thresh, curr_f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3912a2c4a37dc915a010517af6289460249114fb"},"cell_type":"code","source":"ens_test_pred = (0.33 * wiki_pred_Y) + (0.33 * para_pred_Y) + (0.33 * pred_Y)\nens_test_pred = (ens_test_pred>0.35).astype(int)\nens_test_res= pd.DataFrame({\"qid\":test[\"qid\"].values})\nens_test_res['prediction'] = ens_test_pred\nens_test_res.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f474bbf93d2d459118800ba765a30edd5e14c5aa"},"cell_type":"code","source":"len(ens_test_pred[ens_test_pred==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5364cac4ccabe6efd26df863fe09244fa8c13e43"},"cell_type":"code","source":"ens_test_res.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30707cdde37c4d70ad34b0da0a1ed51ba7ac8e43"},"cell_type":"markdown","source":"### Acknowledgments"},{"metadata":{"_uuid":"b6cca48b95a145d1b7cddfad70705afd7668186c"},"cell_type":"markdown","source":"* [http://www.kaggle.com/nikhilroxtomar/playing-with-embeddings-using-lstm-and-cnn](http://www.kaggle.com/nikhilroxtomar/playing-with-embeddings-using-lstm-and-cnn)\n* [https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings](http://https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n* [https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings](http://https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}